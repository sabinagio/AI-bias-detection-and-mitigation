{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from dfply import *\n",
    "from itertools import product\n",
    "# from aif360.metrics import DatasetMetric\n",
    "# from aif360.datasets import StructuredDataset\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from preprocess import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dataset to AIF360 readable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw = pd.read_csv('jigsaw_data_w_racial_descriptors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw['clean_comment'] = jigsaw['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>3.747849e-01</td>\n",
       "      <td>4.840745e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>6.529516e-01</td>\n",
       "      <td>4.760384e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>7.389499e-02</td>\n",
       "      <td>2.616038e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latino</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>2.927915e-02</td>\n",
       "      <td>1.685904e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>9.789380e-04</td>\n",
       "      <td>3.127313e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>1.983105e-02</td>\n",
       "      <td>6.425148e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.232409e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>1.846851e-01</td>\n",
       "      <td>2.078000e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111111e-01</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_attack</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>2.473804e-01</td>\n",
       "      <td>2.145312e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexual_explicit</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>9.536097e-03</td>\n",
       "      <td>4.962459e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.271137e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxicity</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>3.051161e-01</td>\n",
       "      <td>2.511818e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>2.197996e-02</td>\n",
       "      <td>7.292598e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxicity</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>1.736986e-02</td>\n",
       "      <td>4.194758e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.035112e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>9.495699e+00</td>\n",
       "      <td>7.887034e+01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.856000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>1.788045e+01</td>\n",
       "      <td>7.645865e+01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.120000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>2.104123e-01</td>\n",
       "      <td>8.379753e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wow</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>5.882527e-02</td>\n",
       "      <td>2.862638e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>1.334619e-01</td>\n",
       "      <td>5.205030e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likes</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>2.517532e+00</td>\n",
       "      <td>5.000321e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.410000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disagree</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>6.900920e-01</td>\n",
       "      <td>2.226964e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.730000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>4.330531e+06</td>\n",
       "      <td>2.470081e+06</td>\n",
       "      <td>239904.0</td>\n",
       "      <td>929650.5</td>\n",
       "      <td>5.631234e+06</td>\n",
       "      <td>6024207.75</td>\n",
       "      <td>7.194380e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication_id</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>5.196221e+01</td>\n",
       "      <td>3.137175e+01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>55.00</td>\n",
       "      <td>1.050000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent_id</th>\n",
       "      <td>20209.0</td>\n",
       "      <td>3.986320e+06</td>\n",
       "      <td>2.450593e+06</td>\n",
       "      <td>240041.0</td>\n",
       "      <td>827358.0</td>\n",
       "      <td>5.474919e+06</td>\n",
       "      <td>5872028.00</td>\n",
       "      <td>6.333669e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <td>33710.0</td>\n",
       "      <td>2.951282e+05</td>\n",
       "      <td>1.031770e+05</td>\n",
       "      <td>25876.0</td>\n",
       "      <td>162465.0</td>\n",
       "      <td>3.486440e+05</td>\n",
       "      <td>372394.50</td>\n",
       "      <td>3.995010e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count          mean           std       min  \\\n",
       "black                     33710.0  3.747849e-01  4.840745e-01       0.0   \n",
       "white                     33710.0  6.529516e-01  4.760384e-01       0.0   \n",
       "asian                     33710.0  7.389499e-02  2.616038e-01       0.0   \n",
       "latino                    33710.0  2.927915e-02  1.685904e-01       0.0   \n",
       "other_race_or_ethnicity   33710.0  9.789380e-04  3.127313e-02       0.0   \n",
       "threat                    33710.0  1.983105e-02  6.425148e-02       0.0   \n",
       "insult                    33710.0  1.846851e-01  2.078000e-01       0.0   \n",
       "identity_attack           33710.0  2.473804e-01  2.145312e-01       0.0   \n",
       "sexual_explicit           33710.0  9.536097e-03  4.962459e-02       0.0   \n",
       "toxicity                  33710.0  3.051161e-01  2.511818e-01       0.0   \n",
       "obscene                   33710.0  2.197996e-02  7.292598e-02       0.0   \n",
       "severe_toxicity           33710.0  1.736986e-02  4.194758e-02       0.0   \n",
       "identity_annotator_count  33710.0  9.495699e+00  7.887034e+01       4.0   \n",
       "toxicity_annotator_count  33710.0  1.788045e+01  7.645865e+01       4.0   \n",
       "funny                     33710.0  2.104123e-01  8.379753e-01       0.0   \n",
       "wow                       33710.0  5.882527e-02  2.862638e-01       0.0   \n",
       "sad                       33710.0  1.334619e-01  5.205030e-01       0.0   \n",
       "likes                     33710.0  2.517532e+00  5.000321e+00       0.0   \n",
       "disagree                  33710.0  6.900920e-01  2.226964e+00       0.0   \n",
       "id                        33710.0  4.330531e+06  2.470081e+06  239904.0   \n",
       "publication_id            33710.0  5.196221e+01  3.137175e+01       6.0   \n",
       "parent_id                 20209.0  3.986320e+06  2.450593e+06  240041.0   \n",
       "article_id                33710.0  2.951282e+05  1.031770e+05   25876.0   \n",
       "\n",
       "                               25%           50%         75%           max  \n",
       "black                          0.0  0.000000e+00        1.00  1.000000e+00  \n",
       "white                          0.0  1.000000e+00        1.00  1.000000e+00  \n",
       "asian                          0.0  0.000000e+00        0.00  1.000000e+00  \n",
       "latino                         0.0  0.000000e+00        0.00  1.000000e+00  \n",
       "other_race_or_ethnicity        0.0  0.000000e+00        0.00  1.000000e+00  \n",
       "threat                         0.0  0.000000e+00        0.00  9.232409e-01  \n",
       "insult                         0.0  1.111111e-01        0.30  1.000000e+00  \n",
       "identity_attack                0.0  2.000000e-01        0.40  1.000000e+00  \n",
       "sexual_explicit                0.0  0.000000e+00        0.00  9.271137e-01  \n",
       "toxicity                       0.0  3.000000e-01        0.50  1.000000e+00  \n",
       "obscene                        0.0  0.000000e+00        0.00  1.000000e+00  \n",
       "severe_toxicity                0.0  0.000000e+00        0.00  5.035112e-01  \n",
       "identity_annotator_count       4.0  4.000000e+00        6.00  1.856000e+03  \n",
       "toxicity_annotator_count       5.0  1.000000e+01       10.00  3.120000e+03  \n",
       "funny                          0.0  0.000000e+00        0.00  3.300000e+01  \n",
       "wow                            0.0  0.000000e+00        0.00  6.000000e+00  \n",
       "sad                            0.0  0.000000e+00        0.00  1.700000e+01  \n",
       "likes                          0.0  1.000000e+00        3.00  1.410000e+02  \n",
       "disagree                       0.0  0.000000e+00        1.00  1.730000e+02  \n",
       "id                        929650.5  5.631234e+06  6024207.75  7.194380e+06  \n",
       "publication_id                21.0  5.400000e+01       55.00  1.050000e+02  \n",
       "parent_id                 827358.0  5.474919e+06  5872028.00  6.333669e+06  \n",
       "article_id                162465.0  3.486440e+05   372394.50  3.995010e+05  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_names = ['black', 'white', 'latino', 'other_race_or_ethnicity', 'asian']\n",
    "label_names = ['threat', 'insult', 'identity_attack', 'sexual_explicit', 'toxicity', 'obscene', 'severe_toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "black                           0\n",
       "white                           0\n",
       "asian                           0\n",
       "latino                          0\n",
       "other_race_or_ethnicity         0\n",
       "threat                          0\n",
       "insult                          0\n",
       "identity_attack                 0\n",
       "sexual_explicit                 0\n",
       "toxicity                        0\n",
       "obscene                         0\n",
       "severe_toxicity                 0\n",
       "identity_annotator_count        0\n",
       "toxicity_annotator_count        0\n",
       "funny                           0\n",
       "wow                             0\n",
       "sad                             0\n",
       "likes                           0\n",
       "disagree                        0\n",
       "id                              0\n",
       "publication_id                  0\n",
       "parent_id                   13501\n",
       "article_id                      0\n",
       "comment_text                    0\n",
       "split                           0\n",
       "created_date                    0\n",
       "rating                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove parent_id as it's not relevant\n",
    "jigsaw.drop('parent_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_aif = StructuredDataset(jigsaw.select_dtypes(np.number), label_names=label_names, protected_attribute_names=protected_attribute_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'black': 0, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 0, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 0, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 0, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 0, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 0, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 0, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 0, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 0, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 0, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 0, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 0, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 0, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 0, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 0, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 0, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 1, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 1, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 1, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 1, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 1, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 1, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 1, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 1, 'white': 0, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 1, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 1, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 1, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 1, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 1, 'asian': 1}, {'black': 1, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 0}, {'black': 1, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 0, 'asian': 1}, {'black': 1, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 0}, {'black': 1, 'white': 1, 'latino': 1, 'other_race_or_ethnicity': 1, 'asian': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Define the races and their possible values\n",
    "possible_values = [0, 1]\n",
    "\n",
    "# Generate all combinations using itertools.product\n",
    "combinations = list(product(possible_values, repeat=len(races)))\n",
    "\n",
    "# Create a list of dictionaries representing each combination\n",
    "result = [dict(zip(protected_attribute_names, combo)) for combo in combinations]\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [{'black': 0, 'white': 1, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 0}]\n",
    "\n",
    "result.remove({'black': 0, 'white': 0, 'latino': 0, 'other_race_or_ethnicity': 0, 'asian': 0})\n",
    "result.remove(p[0])\n",
    "\n",
    "u = result.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize  # or any other tokenizer\n",
    "\n",
    "# Example: assuming `documents` is a list of preprocessed documents\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in jigsaw.comment_text.to_list()]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_embedding(doc_tokens, model):\n",
    "    # Filter tokens that have word vectors in the model\n",
    "    valid_tokens = [token for token in doc_tokens if token in model.wv.key_to_index]\n",
    "\n",
    "    if not valid_tokens:\n",
    "        return None\n",
    "\n",
    "    # Average the word vectors\n",
    "    doc_embedding = model.wv[valid_tokens].mean(axis=0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = [document_embedding(tokens, model) for tokens in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking gender bias\n",
    "gender_analogy = model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('martin', 0.8641698956489563),\n",
       " ('luther', 0.8388773202896118),\n",
       " ('robert', 0.8063703775405884),\n",
       " ('jr.', 0.7955994606018066),\n",
       " ('joe', 0.7945797443389893),\n",
       " ('byrd', 0.7938011884689331),\n",
       " ('dr.', 0.7926991581916809),\n",
       " ('scott', 0.7764047980308533),\n",
       " ('e.', 0.7758944034576416),\n",
       " ('senator', 0.7704107761383057)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings bias detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json \n",
    "folder_path = 'C:/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Dataset Challenge #4/ResponsibleNLP-main/ResponsibleNLP-main/holistic_bias/dataset/v1.1'\n",
    "\n",
    "fairness = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            data_dict = json.load(file)\n",
    "            dict_name = os.path.splitext(filename)[0]\n",
    "            fairness[dict_name] = data_dict\n",
    "\n",
    "del fairness[\"sentence_templates\"]\n",
    "\n",
    "bias_tags = list(set(list(fairness['descriptors'].keys())+list(fairness['standalone_noun_phrases'].keys())+list(fairness['nouns'].keys())))\n",
    "bias_df = pd.DataFrame(data=None, columns=bias_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ability': {'auditory': [{'descriptor': 'Deaf', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'hard-of-hearing', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'hearing-impaired', 'preference': 'dispreferred'}],\n",
       "  'intellectual_and_developmental': [{'descriptor': 'learning-disabled',\n",
       "    'preference': 'dispreferred'},\n",
       "   {'descriptor': 'retarded', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'slow', 'preference': 'dispreferred'}],\n",
       "  'mobility': [{'descriptor': 'crippled', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'paraplegic', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'paralyzed', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'quadriplegic', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'wheelchair-bound', 'preference': 'dispreferred'}],\n",
       "  'neurological': [{'descriptor': 'autistic', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'dyslexic', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'neuroatypical', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'neurodivergent', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'neurodiverse', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'neurotypical', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'non-neurotypical', 'preference': 'dispreferred'}],\n",
       "  'speech': [{'descriptor': 'aphasic', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'dumb', 'preference': 'dispreferred'},\n",
       "   'non-verbal',\n",
       "   {'descriptor': 'speech-impaired', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'mute', 'preference': 'dispreferred'}],\n",
       "  'visual': [{'descriptor': 'blind', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'four-eyed', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'low-vision', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'vision-impaired', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'visually impaired', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'glasses-wearing', 'preference': 'dispreferred'}],\n",
       "  'other_disabilities': [{'descriptor': 'barren',\n",
       "    'preference': 'dispreferred'},\n",
       "   'diabetic',\n",
       "   'immunocompromised'],\n",
       "  'unspecific': [{'descriptor': 'able-bodied', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'differently abled', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'disabled', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'handicapped', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'nondisabled', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'non-disabled', 'preference': 'reviewed'}]},\n",
       " 'age': {'child': ['adolescent', 'teen', 'teenage', 'teenaged'],\n",
       "  'young': ['young',\n",
       "   'younger',\n",
       "   'twenty-year-old',\n",
       "   '20-year-old',\n",
       "   'twenty-five-year-old',\n",
       "   '25-year-old',\n",
       "   'thirty-year-old',\n",
       "   '30-year-old',\n",
       "   'thirty-five-year-old',\n",
       "   '35-year-old',\n",
       "   'forty-year-old',\n",
       "   '40-year-old',\n",
       "   'twenty-something',\n",
       "   'thirty-something'],\n",
       "  'middle_aged': ['forty-five-year-old',\n",
       "   '45-year-old',\n",
       "   'fifty-year-old',\n",
       "   '50-year-old',\n",
       "   'fifty-five-year-old',\n",
       "   '55-year-old',\n",
       "   'sixty-year-old',\n",
       "   '60-year-old',\n",
       "   'forty-something',\n",
       "   'fifty-something',\n",
       "   'sixty-something',\n",
       "   'middle-aged'],\n",
       "  'old': ['sixty-five-year-old',\n",
       "   '65-year-old',\n",
       "   'seventy-year-old',\n",
       "   '70-year-old',\n",
       "   'seventy-five-year-old',\n",
       "   '75-year-old',\n",
       "   'eighty-year-old',\n",
       "   '80-year-old',\n",
       "   'eighty-five-year-old',\n",
       "   '85-year-old',\n",
       "   'ninety-year-old',\n",
       "   '90-year-old',\n",
       "   'ninety-five-year-old',\n",
       "   '95-year-old',\n",
       "   'seventy-something',\n",
       "   'eighty-something',\n",
       "   'ninety-something',\n",
       "   'octogenarian',\n",
       "   'nonagenarian',\n",
       "   'centenarian',\n",
       "   'older',\n",
       "   'old',\n",
       "   'elderly',\n",
       "   'retired',\n",
       "   'senior',\n",
       "   'senior-citizen',\n",
       "   'young-at-heart',\n",
       "   'spry',\n",
       "   'geriatric',\n",
       "   'mature'],\n",
       "  'adult': ['adult']},\n",
       " 'body_type': {'thin': ['bony',\n",
       "   {'descriptor': 'dainty', 'preference': 'dispreferred'},\n",
       "   'gangly',\n",
       "   'lanky',\n",
       "   'skinny',\n",
       "   'slender',\n",
       "   'slim',\n",
       "   'svelte',\n",
       "   'thin',\n",
       "   'underweight'],\n",
       "  'fit': ['fit',\n",
       "   'in-shape',\n",
       "   'muscular',\n",
       "   'physically fit',\n",
       "   'ripped',\n",
       "   'swole',\n",
       "   'toned',\n",
       "   'well-built',\n",
       "   'strong'],\n",
       "  'overweight': ['chubby',\n",
       "   'chunky',\n",
       "   'curvy',\n",
       "   'fat',\n",
       "   'full-figured',\n",
       "   'heavy',\n",
       "   'heavyset',\n",
       "   'heavy-set',\n",
       "   'heftier',\n",
       "   'hefty',\n",
       "   'mildly overweight',\n",
       "   'morbidly obese',\n",
       "   'obese',\n",
       "   'overweight',\n",
       "   'plump',\n",
       "   'plus-sized',\n",
       "   'potbellied',\n",
       "   'slightly overweight',\n",
       "   'rotund',\n",
       "   'bulky'],\n",
       "  'attractive': ['adorable',\n",
       "   'attractive',\n",
       "   'beautiful',\n",
       "   'cute',\n",
       "   'good-looking',\n",
       "   'gorgeous',\n",
       "   'handsome',\n",
       "   'hot',\n",
       "   'hunky',\n",
       "   'pretty',\n",
       "   'sexy'],\n",
       "  'unattractive': ['homely',\n",
       "   'unattractive',\n",
       "   'ugly',\n",
       "   'hideous',\n",
       "   'plain-looking'],\n",
       "  'large_stature': ['barrel-chested',\n",
       "   'beefy',\n",
       "   'big',\n",
       "   'bigger',\n",
       "   'big-boned',\n",
       "   'brawny',\n",
       "   'burly',\n",
       "   'giant',\n",
       "   'huge',\n",
       "   'large',\n",
       "   'large-stature',\n",
       "   'larger',\n",
       "   'massive',\n",
       "   'stocky',\n",
       "   'tall',\n",
       "   'taller',\n",
       "   'very tall',\n",
       "   'gigantic'],\n",
       "  'medium_stature': ['average-height', 'medium-height', 'medium-stature'],\n",
       "  'small_stature': [{'descriptor': 'littler', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'scrawny', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'petite', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'short', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'shorter', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'small', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'small-stature', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'smaller', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'tiny', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'vertically challenged', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'very short', 'preference': 'dispreferred'}],\n",
       "  'eye_color': ['blue-eyed',\n",
       "   'brown-eyed',\n",
       "   'green-eyed',\n",
       "   'hazel-eyed',\n",
       "   'gray-eyed',\n",
       "   'grey-eyed',\n",
       "   'amber-eyed'],\n",
       "  'hair': ['bald',\n",
       "   'bald-headed',\n",
       "   'balding',\n",
       "   'bearded',\n",
       "   {'descriptor': 'black-haired', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'blond', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'blonde', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'brown-haired', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'brunette', 'preference': 'reviewed'},\n",
       "   'clean-shaven',\n",
       "   {'descriptor': 'coily-haired', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'curly-haired', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'dark-haired', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'dirty-blonde', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'dreadlocked', 'preference': 'polarizing'},\n",
       "   {'descriptor': 'frizzy-haired', 'preference': 'polarizing'},\n",
       "   {'descriptor': 'ginger', 'preference': 'reviewed'},\n",
       "   'goateed',\n",
       "   'gray-haired',\n",
       "   'graying',\n",
       "   'hairy',\n",
       "   {'descriptor': 'kinky-haired', 'preference': 'polarizing'},\n",
       "   {'descriptor': 'light-haired', 'preference': 'reviewed'},\n",
       "   'long-haired',\n",
       "   'mustachioed',\n",
       "   'ponytailed',\n",
       "   {'descriptor': 'redhead', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'redheaded', 'preference': 'reviewed'},\n",
       "   'salt-and-pepper-haired',\n",
       "   'scruffy',\n",
       "   'shaggy',\n",
       "   'short-haired',\n",
       "   'silver-haired',\n",
       "   'spiky-haired',\n",
       "   'unshaved',\n",
       "   'white-haired',\n",
       "   {'descriptor': 'straight-haired', 'preference': 'reviewed'}],\n",
       "  'skin_color': [{'descriptor': 'brown-skinned', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'dark-skinned', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'light-skinned', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'olive-skinned', 'preference': 'reviewed'},\n",
       "   'pale-skinned'],\n",
       "  'miscellaneous': ['acne-covered',\n",
       "   'acne-ridden',\n",
       "   'smelly',\n",
       "   'stinky',\n",
       "   'out-of-shape',\n",
       "   'well-groomed',\n",
       "   'grungy',\n",
       "   'scrubby']},\n",
       " 'characteristics': {'addictions': ['addict',\n",
       "   'alcohol-abusing',\n",
       "   'alcoholic',\n",
       "   'drug-abusing',\n",
       "   'drug-addict',\n",
       "   'drug-addicted',\n",
       "   'druggie',\n",
       "   'junkie'],\n",
       "  'children': ['pregnant', 'childfree', 'childless'],\n",
       "  'domestic_status': ['homeless',\n",
       "   'houseless',\n",
       "   'unhoused',\n",
       "   'nomadic',\n",
       "   'orphan',\n",
       "   'adopted'],\n",
       "  'handedness': ['ambidextrous', 'left-handed', 'right-handed'],\n",
       "  'immigration_status': ['alien',\n",
       "   'American-born',\n",
       "   'asylum-seeking',\n",
       "   'foreign',\n",
       "   'foreign-born',\n",
       "   'immigrant',\n",
       "   'native',\n",
       "   'native-born',\n",
       "   'naturalized',\n",
       "   'non-native',\n",
       "   'nonnative',\n",
       "   'refugee',\n",
       "   'stateless',\n",
       "   'undocumented',\n",
       "   {'descriptor': 'US-born', 'article': 'a'}],\n",
       "  'job_status': ['blue-collar',\n",
       "   'employed',\n",
       "   'laid-off',\n",
       "   'professional',\n",
       "   {'descriptor': 'homemaker', 'preference': 'dispreferred'},\n",
       "   'stay-at-home',\n",
       "   'underemployed',\n",
       "   'unemployed',\n",
       "   'white-collar',\n",
       "   'half-timer',\n",
       "   'part-timer',\n",
       "   'casual worker',\n",
       "   'gig worker',\n",
       "   'independently wealthy'],\n",
       "  'marital_status': ['divorced',\n",
       "   'married',\n",
       "   'separated',\n",
       "   'single',\n",
       "   'unattached',\n",
       "   'affianced',\n",
       "   'engaged',\n",
       "   'unmarried',\n",
       "   'widowed',\n",
       "   'partnered',\n",
       "   'available'],\n",
       "  'miscellaneous': ['athletic',\n",
       "   'unathletic',\n",
       "   'active',\n",
       "   'inactive',\n",
       "   'sedentary',\n",
       "   'literate',\n",
       "   'illiterate']},\n",
       " 'cultural': {'generations': ['Greatest Generation',\n",
       "   'Baby Boomer',\n",
       "   'Boomer',\n",
       "   'Gen-X',\n",
       "   'Millennial',\n",
       "   'Gen-Z',\n",
       "   'Zoomer'],\n",
       "  'cultural_descriptors': ['bougie',\n",
       "   'goth',\n",
       "   'hillbilly',\n",
       "   'hippie',\n",
       "   'hippy',\n",
       "   'hipster',\n",
       "   'redneck',\n",
       "   'yuppie',\n",
       "   'underprivileged',\n",
       "   {'descriptor': 'at-risk', 'preference': 'dispreferred'},\n",
       "   'marginalized',\n",
       "   'underserved',\n",
       "   'privileged',\n",
       "   'elite',\n",
       "   {'descriptor': 'entitled', 'preference': 'dispreferred'},\n",
       "   'NIMBY',\n",
       "   'cosmopolitan',\n",
       "   'urban',\n",
       "   'rural',\n",
       "   'country',\n",
       "   'pro-vaccine',\n",
       "   'anti-vaccine',\n",
       "   'anti-vax',\n",
       "   'vaccine-hesitant',\n",
       "   'pro-choice',\n",
       "   'pro-life']},\n",
       " 'gender_and_sex': {'binary': [{'descriptor': 'female', 'gender': 'female'},\n",
       "   {'descriptor': 'male', 'gender': 'male'},\n",
       "   'gender-conforming'],\n",
       "  'cisgender': ['cis', 'cisgender'],\n",
       "  'descriptors': ['AFAB',\n",
       "   'AMAB',\n",
       "   'androgynous',\n",
       "   'butch',\n",
       "   'effeminate',\n",
       "   'feminine',\n",
       "   'femme',\n",
       "   {'descriptor': 'fruitcake', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'fruity', 'preference': 'dispreferred'},\n",
       "   'macho',\n",
       "   'manly',\n",
       "   'masculine',\n",
       "   'womanly'],\n",
       "  'non_binary_or_gnc': ['agender',\n",
       "   'androgyne',\n",
       "   'bigender',\n",
       "   'boi',\n",
       "   'demiboy',\n",
       "   'demigender',\n",
       "   'demigirl',\n",
       "   'enby',\n",
       "   'feminine-of-center',\n",
       "   'FoC',\n",
       "   'gender neutral',\n",
       "   'gender non-conforming',\n",
       "   'genderfluid',\n",
       "   'genderless',\n",
       "   'genderqueer',\n",
       "   'masculine-of-center',\n",
       "   'MoC',\n",
       "   'neutrois',\n",
       "   'nonbinary',\n",
       "   'non-binary',\n",
       "   'stud',\n",
       "   'third-gender',\n",
       "   'tomboy',\n",
       "   'two-spirit'],\n",
       "  'queer': ['LGBT', 'LGBTQ', 'LGBTQ+', 'queer'],\n",
       "  'sex': ['hermaphrodite', 'intersex', 'endosex', 'transsexual'],\n",
       "  'transgender': [{'descriptor': 'FTM', 'gender': 'male'},\n",
       "   {'descriptor': 'F2M', 'gender': 'male'},\n",
       "   {'descriptor': 'MTF', 'gender': 'female'},\n",
       "   {'descriptor': 'M2F', 'gender': 'female'},\n",
       "   'trans',\n",
       "   {'descriptor': 'trans female', 'gender': 'female'},\n",
       "   'trans feminine',\n",
       "   'trans fem',\n",
       "   'trans femme',\n",
       "   {'descriptor': 'trans male', 'gender': 'male'},\n",
       "   'trans masculine',\n",
       "   'trans masc',\n",
       "   'transfeminine',\n",
       "   'transgender',\n",
       "   'transmasculine']},\n",
       " 'nationality': {'china': [{'descriptor': 'Chinese', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Chinese-American', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'American-Born Chinese', 'preference': 'reviewed'}],\n",
       "  'cuba': [{'descriptor': 'Cuban', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Cuban-American', 'preference': 'reviewed'}],\n",
       "  'dominican_republic': [{'descriptor': 'Dominican', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Dominican-American', 'preference': 'reviewed'}],\n",
       "  'el_salvador': [{'descriptor': 'Salvadoran', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Salvadoran-American', 'preference': 'reviewed'}],\n",
       "  'guatemala': [{'descriptor': 'Guatemalan', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Guatemalan-American', 'preference': 'reviewed'}],\n",
       "  'india': [{'descriptor': 'Indian', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Indian-American', 'preference': 'reviewed'}],\n",
       "  'mexico': [{'descriptor': 'Mexican', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Mexican-American', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Chicana', 'gender': 'female'},\n",
       "   {'descriptor': 'Chicano', 'gender': 'male'}],\n",
       "  'philippines': [{'descriptor': 'Filipina',\n",
       "    'gender': 'female',\n",
       "    'preference': 'reviewed'},\n",
       "   {'descriptor': 'Filipina-American',\n",
       "    'gender': 'female',\n",
       "    'preference': 'reviewed'},\n",
       "   {'descriptor': 'Filipino', 'gender': 'male', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Filipino-American',\n",
       "    'gender': 'male',\n",
       "    'preference': 'reviewed'}],\n",
       "  'south_korea': [{'descriptor': 'Korean', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Korean-American', 'preference': 'reviewed'}],\n",
       "  'united_states': ['American'],\n",
       "  'vietnam': [{'descriptor': 'Vietnamese', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Vietnamese-American', 'preference': 'reviewed'}]},\n",
       " 'nonce': {'all': ['blicket',\n",
       "   'stad',\n",
       "   'mell',\n",
       "   'coodle',\n",
       "   'doff',\n",
       "   'tannin',\n",
       "   'fitch',\n",
       "   'tulver']},\n",
       " 'political_ideologies': {'all': ['liberal',\n",
       "   'conservative',\n",
       "   'Democrat',\n",
       "   'Republican',\n",
       "   'leftist',\n",
       "   'rightist',\n",
       "   'left-wing',\n",
       "   'right-wing',\n",
       "   'socially liberal',\n",
       "   'fiscally conservative',\n",
       "   'democratic-socialist',\n",
       "   'centrist',\n",
       "   'independent',\n",
       "   'moderate',\n",
       "   'libertarian',\n",
       "   'socialist',\n",
       "   'communist',\n",
       "   'progressive',\n",
       "   'radical',\n",
       "   'nationalist',\n",
       "   'apolitical',\n",
       "   'anarchist',\n",
       "   'extremist']},\n",
       " 'race_ethnicity': {'alaska_native': ['Alaska Native'],\n",
       "  'asian': [{'descriptor': 'Asian', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Asian-American', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Desi', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'East Asian', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Oriental', 'preference': 'dispreferred'},\n",
       "   {'descriptor': 'South Asian', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Southeast Asian', 'preference': 'reviewed'}],\n",
       "  'black': [{'descriptor': 'African', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'African-American', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Black', 'preference': 'reviewed'}],\n",
       "  'latinx': [{'descriptor': 'Hispanic', 'preference': 'reviewed'},\n",
       "   'Latin',\n",
       "   {'descriptor': 'Latinx', 'preference': 'polarizing'},\n",
       "   {'descriptor': 'Latine', 'preference': 'polarizing'},\n",
       "   {'descriptor': 'Latina', 'gender': 'female', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Latino', 'gender': 'male', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'Latin American', 'preference': 'reviewed'}],\n",
       "  'indigenous': ['Aboriginal',\n",
       "   'Indigenous',\n",
       "   'Native American',\n",
       "   'American Indian'],\n",
       "  'native_hawaiian': ['Native Hawaiian'],\n",
       "  'pacific_islander': [{'descriptor': 'Pacific Islander',\n",
       "    'preference': 'reviewed'}],\n",
       "  'white': ['European',\n",
       "   'European-American',\n",
       "   'Middle Eastern',\n",
       "   'white',\n",
       "   'Caucasian',\n",
       "   'Arab'],\n",
       "  'combined': [{'descriptor': 'AAPI', 'preference': 'reviewed'},\n",
       "   {'descriptor': 'BIPOC', 'preference': 'reviewed'}]},\n",
       " 'religion': {'all': ['agnostic',\n",
       "   'Amish',\n",
       "   'Ashkenazi Jewish',\n",
       "   'atheist',\n",
       "   \"Bahá'í\",\n",
       "   'born-again',\n",
       "   'Buddhist',\n",
       "   'Catholic',\n",
       "   'Cheondoist',\n",
       "   'Christian',\n",
       "   'Confucianist',\n",
       "   'Conservative Jewish',\n",
       "   'Druze',\n",
       "   'Evangelical',\n",
       "   'Hasidic',\n",
       "   'Hindu',\n",
       "   'humanist',\n",
       "   'irreligious',\n",
       "   'Jain',\n",
       "   \"Jehovah's Witness\",\n",
       "   'Jewish',\n",
       "   'Lutheran',\n",
       "   'Methodist',\n",
       "   'Mormon',\n",
       "   'Muslim',\n",
       "   'Orthodox Jewish',\n",
       "   'Pagan',\n",
       "   'Protestant',\n",
       "   'Rasta',\n",
       "   'Rastafari',\n",
       "   'Rastafarian',\n",
       "   'Reform Jewish',\n",
       "   'religious',\n",
       "   'Roman Catholic',\n",
       "   'secular',\n",
       "   'Satanist',\n",
       "   'Scientologist',\n",
       "   'Sephardic Jewish',\n",
       "   'Seventh-Day Adventist',\n",
       "   'Shia',\n",
       "   'Shintoist',\n",
       "   'Sikh',\n",
       "   'spiritual',\n",
       "   'Spiritualist',\n",
       "   'Sunni',\n",
       "   'Taoist',\n",
       "   'Wiccan',\n",
       "   {'descriptor': 'Unitarian', 'article': 'a'},\n",
       "   'Zoroastrian']},\n",
       " 'sexual_orientation': {'asexual_or_aromantic': ['asexual',\n",
       "   'ace',\n",
       "   'aromantic'],\n",
       "  'bi': ['bi', 'bisexual'],\n",
       "  'demisexual': ['demisexual'],\n",
       "  'gay': ['gay', 'homosexual'],\n",
       "  'lesbian': [{'descriptor': 'lesbian', 'gender': 'female'}],\n",
       "  'pansexual': ['pan', 'pansexual'],\n",
       "  'polyamorous': ['polyamorous', 'poly'],\n",
       "  'queer': ['LGBT', 'LGBTQ', 'LGBTQ+', 'queer'],\n",
       "  'straight': ['straight', 'hetero', 'heterosexual']},\n",
       " 'socioeconomic_class': {'upper_class': ['affluent',\n",
       "   'financially well-off',\n",
       "   'high-net-worth',\n",
       "   'moneyed',\n",
       "   'rich',\n",
       "   'one-percenter',\n",
       "   'upper-class',\n",
       "   'wealthy',\n",
       "   'well-to-do',\n",
       "   'well-off'],\n",
       "  'middle_class': ['middle-class'],\n",
       "  'working_class': ['working-class'],\n",
       "  'below_poverty_line': ['poor', 'broke', 'low-income'],\n",
       "  'educational_attainment': ['high-school-dropout', 'college-graduate']}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness['descriptors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(?:a|an)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "nationality = fairness['descriptors']['nationality']\n",
    "nationality = [entry['descriptor'] for descriptors_list in nationality.values() for entry in descriptors_list if 'descriptor' in entry]\n",
    "\n",
    "race_ethnicity = fairness['descriptors']['race_ethnicity']\n",
    "race_ethnicity_snp = fairness['standalone_noun_phrases']['race_ethnicity']\n",
    "race_ethnicity_1 = [entry['descriptor'] for descriptors_list in race_ethnicity.values() for entry in descriptors_list if 'descriptor' in entry]\n",
    "race_ethnicity_2 = [re.sub(pattern, '', value).strip() for entry in race_ethnicity_snp for value in entry.values()]\n",
    "race_ethnicity = race_ethnicity_1 + race_ethnicity_2\n",
    "\n",
    "religion = fairness['descriptors']['religion']\n",
    "religion_snp = fairness['standalone_noun_phrases']['religion']\n",
    "religion = ['Ashkenazi Jewish', \"Bahá'í\", 'Buddhist','Cheondoist', 'Confucianist', 'Conservative Jewish', 'Druze',\n",
    "        'Hasidic', 'Hindu', 'Jain', 'Jewish', 'Muslim', 'Orthodox Jewish', 'Rasta', 'Rastafari', 'Rastafarian', 'Reform Jewish',\n",
    "            'Sephardic Jewish', 'Shia', 'Shintoist', 'Sikh', 'Sunni', 'Taoist', 'Zoroastrian', 'Jewish American Princess',\n",
    "                'Jewish American Princesses', 'JAP', 'JAPs']\n",
    "\n",
    "hair = ['dreadlocked', 'curly-haired', 'frizzy-haired', 'coily-haired', 'afro', 'afros', 'jewfro', 'jewfros']\n",
    "skin_color = ['brown-skinned', 'dark-skinned', 'olive-skinned', 'yellow']\n",
    "immigration = ['asylum seeker', 'asylum seekers', 'refugee', 'refugees', 'immigrant', 'immigrants', 'DACA', 'Dreamer', 'Dreamers']\n",
    "\n",
    "racial_indicators = nationality + race_ethnicity + religion + hair + skin_color + immigration\n",
    "racial_indicators = [indicator.lower() for indicator in racial_indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person of color',\n",
       " 'people of color',\n",
       " 'reviewed',\n",
       " 'man of color',\n",
       " 'men of color',\n",
       " 'woman of color',\n",
       " 'women of color']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_ethnicity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethnicity_1 = {}\n",
    "for race_class, race_info in race_ethnicity.items():\n",
    "    race_ethnicity_1[race_class] = []\n",
    "    for detail in race_info:\n",
    "        if isinstance(detail, str):\n",
    "            race_ethnicity_1[race_class].append(detail)\n",
    "        else:\n",
    "            race_ethnicity_1[race_class].append(detail['descriptor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alaska_native': ['Alaska Native'],\n",
       " 'asian': ['Asian',\n",
       "  'Asian-American',\n",
       "  'Desi',\n",
       "  'East Asian',\n",
       "  'Oriental',\n",
       "  'South Asian',\n",
       "  'Southeast Asian'],\n",
       " 'black': ['African', 'African-American', 'Black'],\n",
       " 'latinx': ['Hispanic',\n",
       "  'Latin',\n",
       "  'Latinx',\n",
       "  'Latine',\n",
       "  'Latina',\n",
       "  'Latino',\n",
       "  'Latin American'],\n",
       " 'indigenous': ['Aboriginal',\n",
       "  'Indigenous',\n",
       "  'Native American',\n",
       "  'American Indian'],\n",
       " 'native_hawaiian': ['Native Hawaiian'],\n",
       " 'pacific_islander': ['Pacific Islander'],\n",
       " 'white': ['European',\n",
       "  'European-American',\n",
       "  'Middle Eastern',\n",
       "  'white',\n",
       "  'Caucasian',\n",
       "  'Arab'],\n",
       " 'combined': ['AAPI',\n",
       "  'BIPOC',\n",
       "  'person of color',\n",
       "  'people of color',\n",
       "  'man of color',\n",
       "  'men of color',\n",
       "  'woman of color',\n",
       "  'women of color']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_ethnicity_2.remove('reviewed')\n",
    "race_ethnicity_1['combined'].extend(race_ethnicity_2)\n",
    "race_ethnicity_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'reviewed' in race_ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'reviewed' in racial_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Gender-related word sets\n",
    "target_words_white =  ['European',\n",
    "  'European-American',\n",
    "  'Middle Eastern',\n",
    "  'white',\n",
    "  'Caucasian',\n",
    "  'Arab']\n",
    "target_words_black = ['African', 'African-American', 'Black']\n",
    "target_words_combined = ['AAPI',\n",
    "  'BIPOC',\n",
    "  'person of color',\n",
    "  'people of color',\n",
    "  'man of color',\n",
    "  'men of color',\n",
    "  'woman of color',\n",
    "  'women of color']\n",
    "\n",
    "# Example: Career and family-related word sets\n",
    "target_words_pleasant = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\", \"happy\", \"laughter\", \"paradise\", \"vacation\", \"joy\", \"love\", \"peace\", \"wonderful\", \"pleasure\", \"friend\", \"laughter\", \"happy\"]\n",
    "target_words_unpleasant = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"bomb\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"evil\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"terrible\", \"horrible\", \"nasty\", \"evil\", \"war\", \"awful\", \"failure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, model):\n",
    "    word = word.lower().strip()\n",
    "    if word in model.wv:\n",
    "        embedding = model.wv[word]\n",
    "        return embedding\n",
    "    else:\n",
    "        print(f\"No embedding found for '{word}' in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def weat_effect_size(X, Y, A, B, model):\n",
    "    X = get_word_embedding(X, model)\n",
    "    Y = get_word_embedding(Y, model)\n",
    "    A = get_word_embedding(A, model)\n",
    "    B = get_word_embedding(B, model)\n",
    "    \n",
    "    mean_XY = (pairwise_distances(X, Y, metric='cosine')).mean()\n",
    "    mean_AB = (pairwise_distances(A, B, metric='cosine')).mean()\n",
    "    mean_AX = (pairwise_distances(A, X, metric='cosine')).mean()\n",
    "    mean_BX = (pairwise_distances(B, X, metric='cosine')).mean()\n",
    "\n",
    "    effect_size = (mean_XY - mean_AB) / ((mean_AX + mean_BX) / 2)\n",
    "    return effect_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weat_permutation_test(X, Y, A, B, model, num_permutations=1000):\n",
    "    target = X + Y\n",
    "    attribute = A + B\n",
    "\n",
    "    observed_effect_size = weat_effect_size(X, Y, A, B, model)\n",
    "\n",
    "    combined = target + attribute\n",
    "    num_samples = len(combined)\n",
    "\n",
    "    # Run permutation test\n",
    "    effect_sizes = np.zeros(num_permutations)\n",
    "    for i in range(num_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        perm_X = combined[:len(X)]\n",
    "        perm_Y = combined[len(X):len(X) + len(Y)]\n",
    "        perm_A = combined[len(X) + len(Y):len(X) + len(Y) + len(A)]\n",
    "        perm_B = combined[len(X) + len(Y) + len(A):]\n",
    "\n",
    "        effect_sizes[i] = weat_effect_size(perm_X, perm_Y, perm_A, perm_B, model)\n",
    "\n",
    "    p_value = (np.abs(effect_sizes) > np.abs(observed_effect_size)).mean()\n",
    "    return observed_effect_size, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No embedding found for 'middle eastern' in the model.\n",
      "No embedding found for 'caress' in the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabin\\AppData\\Local\\Temp\\ipykernel_22076\\787654814.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array([get_word_embedding(word, model) for word in X])\n",
      "C:\\Users\\sabin\\AppData\\Local\\Temp\\ipykernel_22076\\787654814.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  A = np.array([get_word_embedding(word, model) for word in A])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m weat_effect_size(target_words_white, target_words_black, target_words_pleasant, target_words_unpleasant, model)\n",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y106sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_word_embedding(word, model) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m A])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y106sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m B \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_word_embedding(word, model) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m B])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y106sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m mean_XY \u001b[39m=\u001b[39m (pairwise_distances(X, Y, metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcosine\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y106sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m mean_AB \u001b[39m=\u001b[39m (pairwise_distances(A, B, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y106sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m mean_AX \u001b[39m=\u001b[39m (pairwise_distances(A, X, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2039\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m   2037\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m-> 2039\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1579\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1578\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 1579\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m   1581\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1000\u001b[0m, in \u001b[0;36mcosine_distances\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine distance between samples in X and Y.\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \n\u001b[0;32m    976\u001b[0m \u001b[39mCosine distance is defined as 1.0 minus the cosine similarity.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[39mscipy.spatial.distance.cosine : Dense matrices only.\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[39m# 1.0 - cosine_similarity(X, Y) without copy\u001b[39;00m\n\u001b[1;32m-> 1000\u001b[0m S \u001b[39m=\u001b[39m cosine_similarity(X, Y)\n\u001b[0;32m   1001\u001b[0m S \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   1002\u001b[0m S \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m \n\u001b[0;32m   1360\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1393\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m   1395\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:155\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    147\u001b[0m         X,\n\u001b[0;32m    148\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    156\u001b[0m         X,\n\u001b[0;32m    157\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    158\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    159\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    160\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    161\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    164\u001b[0m         Y,\n\u001b[0;32m    165\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    170\u001b[0m     )\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sabin\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "weat_effect_size(target_words_white, target_words_black, target_words_pleasant, target_words_unpleasant, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['European',\n",
       " 'European-American',\n",
       " 'Middle Eastern',\n",
       " 'white',\n",
       " 'Caucasian',\n",
       " 'Arab']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['African', 'African-American', 'Black']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No embedding found for 'middle eastern' in the model.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m weat_effect_size(target_words_white, target_words_black, target_words_pleasant, target_words_unpleasant, model)\n",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweat_effect_size\u001b[39m(X, Y, A, B, model):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweat_effect_size\u001b[39m(X, Y, A, B, model):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "weat_effect_size(target_words_white, target_words_black, target_words_pleasant, target_words_unpleasant, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No embedding found for 'middle eastern' in the model.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m observed_effect_size, p_value\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Example: Run WEAT test for gender bias\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m observed_effect_size, p_value \u001b[39m=\u001b[39m weat_permutation_test(target_words_white, target_words_black, target_words_pleasant, target_words_unpleasant, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObserved Effect Size: \u001b[39m\u001b[39m{\u001b[39;00mobserved_effect_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mP-value: \u001b[39m\u001b[39m{\u001b[39;00mp_value\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m target \u001b[39m=\u001b[39m X \u001b[39m+\u001b[39m Y\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m attribute \u001b[39m=\u001b[39m A \u001b[39m+\u001b[39m B\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m observed_effect_size \u001b[39m=\u001b[39m weat_effect_size(X, Y, A, B, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m combined \u001b[39m=\u001b[39m target \u001b[39m+\u001b[39m attribute\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(combined)\n",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweat_effect_size\u001b[39m(X, Y, A, B, model):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n",
      "\u001b[1;32mc:\\Users\\sabin\\Downloads\\freelancing\\permutable.ai\\01_handover\\00_projects\\08_marketing\\hackathon\\Jigsaw_puzzle\\bias_analysis.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweat_effect_size\u001b[39m(X, Y, A, B, model):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Jigsaw_puzzle/bias_analysis.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([emb \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m X \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m get_word_embedding(word, model) \u001b[39mif\u001b[39;00m emb])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "def get_word_embedding(word, model):\n",
    "    word_lower = word.lower().strip()\n",
    "    if word_lower in model.wv:\n",
    "        embedding = model.wv[word_lower]\n",
    "        return embedding.reshape(-1, 1)\n",
    "    else:\n",
    "        print(f\"No embedding found for '{word_lower}' in the model.\")\n",
    "        return None\n",
    "\n",
    "def weat_effect_size(X, Y, A, B, model):\n",
    "    X = np.array([get_word_embedding(word, model) for word in X if get_word_embedding(word, model)])\n",
    "    Y = np.array([get_word_embedding(word, model) for word in Y if get_word_embedding(word, model)])\n",
    "    A = np.array([get_word_embedding(word, model) for word in A if get_word_embedding(word, model)])\n",
    "    B = np.array([get_word_embedding(word, model) for word in B if get_word_embedding(word, model)])\n",
    "\n",
    "    mean_XY = (pairwise_distances(X, Y, metric='cosine')).mean()\n",
    "    mean_AB = (pairwise_distances(A, B, metric='cosine')).mean()\n",
    "    mean_AX = (pairwise_distances(A, X, metric='cosine')).mean()\n",
    "    mean_BX = (pairwise_distances(B, X, metric='cosine')).mean()\n",
    "\n",
    "    effect_size = (mean_XY - mean_AB) / ((mean_AX + mean_BX) / 2)\n",
    "    return effect_size\n",
    "\n",
    "def weat_permutation_test(X, Y, A, B, model, num_permutations=1000):\n",
    "    target = X + Y\n",
    "    attribute = A + B\n",
    "\n",
    "    observed_effect_size = weat_effect_size(X, Y, A, B, model)\n",
    "\n",
    "    combined = target + attribute\n",
    "    num_samples = len(combined)\n",
    "\n",
    "    # Run permutation test\n",
    "    effect_sizes = np.zeros(num_permutations)\n",
    "    for i in range(num_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        perm_X = combined[:len(X)]\n",
    "        perm_Y = combined[len(X):len(X) + len(Y)]\n",
    "        perm_A = combined[len(X) + len(Y):len(X) + len(Y) + len(A)]\n",
    "        perm_B = combined[len(X) + len(Y) + len(A):]\n",
    "\n",
    "        effect_sizes[i] = weat_effect_size(perm_X, perm_Y, perm_A, perm_B, model)\n",
    "\n",
    "    p_value = (np.abs(effect_sizes) > np.abs(observed_effect_size)).mean()\n",
    "    return observed_effect_size, p_value\n",
    "\n",
    "# Example: Run WEAT test for gender bias\n",
    "observed_effect_size, p_value = weat_permutation_test(target_words_white, target_words_black, target_words_pleasant, target_words_unpleasant, model)\n",
    "\n",
    "print(f'Observed Effect Size: {observed_effect_size}')\n",
    "print(f'P-value: {p_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check bias with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>id</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5892815</td>\n",
       "      <td>54</td>\n",
       "      <td>373428</td>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-03 23:20:08.226613+00</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>416437</td>\n",
       "      <td>21</td>\n",
       "      <td>143025</td>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-08-04 16:48:07.175252+00</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>377096</td>\n",
       "      <td>21</td>\n",
       "      <td>140782</td>\n",
       "      <td>\"Let's get the black folks and the white folks...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-07-06 16:49:00.967646+00</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5697567</td>\n",
       "      <td>21</td>\n",
       "      <td>361553</td>\n",
       "      <td>Are you a Pilgrim?\\nWhy arn't you growing your...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-08-01 18:39:14.212043+00</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7160163</td>\n",
       "      <td>102</td>\n",
       "      <td>367562</td>\n",
       "      <td>And there it is. Our president is a white supr...</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-08-17 16:19:08.435159+00</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  white  asian  latino  other_race_or_ethnicity    threat    insult  \\\n",
       "0      1      0      0       0                        0  0.000000  0.642857   \n",
       "1      1      0      0       0                        0  0.000000  0.213115   \n",
       "2      1      1      0       0                        0  0.250000  0.236842   \n",
       "3      0      1      0       0                        0  0.000000  0.500000   \n",
       "4      0      1      0       0                        0  0.014085  0.323944   \n",
       "\n",
       "   identity_attack  sexual_explicit  toxicity   obscene  severe_toxicity  \\\n",
       "0         0.614286         0.014286  0.757143  0.471429         0.142857   \n",
       "1         0.639344         0.000000  0.688525  0.049180         0.032787   \n",
       "2         0.631579         0.013158  0.736842  0.000000         0.078947   \n",
       "3         0.400000         0.000000  0.800000  0.000000         0.000000   \n",
       "4         0.323944         0.000000  0.507042  0.000000         0.000000   \n",
       "\n",
       "   identity_annotator_count  toxicity_annotator_count  funny  wow  sad  likes  \\\n",
       "0                         4                        70      0    0    0      0   \n",
       "1                         4                        61      0    0    0      1   \n",
       "2                         4                        76      0    0    0      2   \n",
       "3                         4                        10      0    0    0      1   \n",
       "4                         4                        71      0    1    0      7   \n",
       "\n",
       "   disagree       id  publication_id  article_id  \\\n",
       "0         0  5892815              54      373428   \n",
       "1         0   416437              21      143025   \n",
       "2         0   377096              21      140782   \n",
       "3         0  5697567              21      361553   \n",
       "4         3  7160163             102      367562   \n",
       "\n",
       "                                        comment_text  split  \\\n",
       "0  Why is this black racist crap still on the G&M...  train   \n",
       "1                         even up here.......BLACKS!  train   \n",
       "2  \"Let's get the black folks and the white folks...  train   \n",
       "3  Are you a Pilgrim?\\nWhy arn't you growing your...  train   \n",
       "4  And there it is. Our president is a white supr...   test   \n",
       "\n",
       "                    created_date    rating  \n",
       "0  2017-09-03 23:20:08.226613+00  rejected  \n",
       "1  2016-08-04 16:48:07.175252+00  rejected  \n",
       "2  2016-07-06 16:49:00.967646+00  approved  \n",
       "3  2017-08-01 18:39:14.212043+00  approved  \n",
       "4  2017-08-17 16:19:08.435159+00  approved  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "approved    29057\n",
       "rejected     4653\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    30658\n",
       "test      3052\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jigsaw[protected_attribute_names]\n",
    "y_train = jigsaw[jigsaw['split'] == 'train'] >> select('rating')\n",
    "y_test = jigsaw[jigsaw['split'] == 'test']  >> select('rating')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
