{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "from preprocess import *\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "`count` = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "`hate_speech` = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "`offensive_language` = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "`neither` = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "`class` = class label for majority of CF users.\n",
    "  0 - hate speech\n",
    "  1 - offensive  language\n",
    "  2 - neither\n",
    "\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('labeled_data.csv', index_col=False).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# read the json \n",
    "folder_path = 'C:/Users/sabin/Downloads/freelancing/permutable.ai/01_handover/00_projects/08_marketing/hackathon/Dataset Challenge #4/ResponsibleNLP-main/ResponsibleNLP-main/holistic_bias/dataset/v1.1'\n",
    "fairness = {}\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data_dict = json.load(file)\n",
    "            dict_name = os.path.splitext(filename)[0]\n",
    "            fairness[dict_name] = data_dict\n",
    "del fairness[\"sentence_templates\"]\n",
    "bias_tags = list(set(list(fairness['descriptors'].keys())+list(fairness['standalone_noun_phrases'].keys())+list(fairness['nouns'].keys())))\n",
    "bias_df = pd.DataFrame(data=None, columns=bias_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = re.compile(r'\\b(?:a|an)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "nationality = fairness['descriptors']['nationality']\n",
    "nationality = [entry['descriptor'] for descriptors_list in nationality.values() for entry in descriptors_list if 'descriptor' in entry]\n",
    "\n",
    "race_ethnicity = fairness['descriptors']['race_ethnicity']\n",
    "race_ethnicity_snp = fairness['standalone_noun_phrases']['race_ethnicity']\n",
    "race_ethnicity_1 = [entry['descriptor'] for descriptors_list in race_ethnicity.values() for entry in descriptors_list if 'descriptor' in entry]\n",
    "race_ethnicity_2 = [re.sub(pattern, '', value).strip() for entry in race_ethnicity_snp for value in entry.values() if value != 'reviewed']\n",
    "race_ethnicity = race_ethnicity_1 + race_ethnicity_2\n",
    "\n",
    "religion = fairness['descriptors']['religion']\n",
    "religion_snp = fairness['standalone_noun_phrases']['religion']\n",
    "religion = ['Ashkenazi Jewish', \"Bahá'í\", 'Buddhist','Cheondoist', 'Confucianist', 'Conservative Jewish', 'Druze',\n",
    "        'Hasidic', 'Hindu', 'Jain', 'Jewish', 'Muslim', 'Orthodox Jewish', 'Rasta', 'Rastafari', 'Rastafarian', 'Reform Jewish',\n",
    "            'Sephardic Jewish', 'Shia', 'Shintoist', 'Sikh', 'Sunni', 'Taoist', 'Zoroastrian', 'Jewish American Princess',\n",
    "                'Jewish American Princesses', 'JAP', 'JAPs']\n",
    "\n",
    "hair = ['dreadlocked', 'curly-haired', 'frizzy-haired', 'coily-haired', 'afro', 'afros', 'jewfro', 'jewfros']\n",
    "skin_color = ['brown-skinned', 'dark-skinned', 'olive-skinned', 'yellow']\n",
    "immigration = ['asylum seeker', 'asylum seekers', 'refugee', 'refugees', 'immigrant', 'immigrants', 'DACA', 'Dreamer', 'Dreamers']\n",
    "\n",
    "racial_indicators = nationality + race_ethnicity + religion + hair + skin_color + immigration\n",
    "racial_indicators = [indicator.lower() for indicator in racial_indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese',\n",
       " 'chinese-american',\n",
       " 'american-born chinese',\n",
       " 'cuban',\n",
       " 'cuban-american',\n",
       " 'dominican',\n",
       " 'dominican-american',\n",
       " 'salvadoran',\n",
       " 'salvadoran-american',\n",
       " 'guatemalan',\n",
       " 'guatemalan-american',\n",
       " 'indian',\n",
       " 'indian-american',\n",
       " 'mexican',\n",
       " 'mexican-american',\n",
       " 'chicana',\n",
       " 'chicano',\n",
       " 'filipina',\n",
       " 'filipina-american',\n",
       " 'filipino',\n",
       " 'filipino-american',\n",
       " 'korean',\n",
       " 'korean-american',\n",
       " 'vietnamese',\n",
       " 'vietnamese-american',\n",
       " 'asian',\n",
       " 'asian-american',\n",
       " 'desi',\n",
       " 'east asian',\n",
       " 'oriental',\n",
       " 'south asian',\n",
       " 'southeast asian',\n",
       " 'african',\n",
       " 'african-american',\n",
       " 'black',\n",
       " 'hispanic',\n",
       " 'latinx',\n",
       " 'latine',\n",
       " 'latina',\n",
       " 'latino',\n",
       " 'latin american',\n",
       " 'pacific islander',\n",
       " 'aapi',\n",
       " 'bipoc',\n",
       " 'person of color',\n",
       " 'people of color',\n",
       " 'man of color',\n",
       " 'men of color',\n",
       " 'woman of color',\n",
       " 'women of color',\n",
       " 'ashkenazi jewish',\n",
       " \"bahá'í\",\n",
       " 'buddhist',\n",
       " 'cheondoist',\n",
       " 'confucianist',\n",
       " 'conservative jewish',\n",
       " 'druze',\n",
       " 'hasidic',\n",
       " 'hindu',\n",
       " 'jain',\n",
       " 'jewish',\n",
       " 'muslim',\n",
       " 'orthodox jewish',\n",
       " 'rasta',\n",
       " 'rastafari',\n",
       " 'rastafarian',\n",
       " 'reform jewish',\n",
       " 'sephardic jewish',\n",
       " 'shia',\n",
       " 'shintoist',\n",
       " 'sikh',\n",
       " 'sunni',\n",
       " 'taoist',\n",
       " 'zoroastrian',\n",
       " 'jewish american princess',\n",
       " 'jewish american princesses',\n",
       " 'jap',\n",
       " 'japs',\n",
       " 'dreadlocked',\n",
       " 'curly-haired',\n",
       " 'frizzy-haired',\n",
       " 'coily-haired',\n",
       " 'afro',\n",
       " 'afros',\n",
       " 'jewfro',\n",
       " 'jewfros',\n",
       " 'brown-skinned',\n",
       " 'dark-skinned',\n",
       " 'olive-skinned',\n",
       " 'yellow',\n",
       " 'asylum seeker',\n",
       " 'asylum seekers',\n",
       " 'refugee',\n",
       " 'refugees',\n",
       " 'immigrant',\n",
       " 'immigrants',\n",
       " 'daca',\n",
       " 'dreamer',\n",
       " 'dreamers']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "racial_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('labeled_data_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sabin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>protected_attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>&amp;#9733; BEST ASIAN MASSAGE ON THE park slope --TOP RATED SPA &amp;#9733; 718-622-0221 - 24 http://t.co/Br7el4ZXrw</td>\n",
       "      <td>best asian massage park slope top rated spa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17144</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Runyacheckup: Niggas and bitches gotta quit wit da money pics... If I can count it accurately it ain't enough &amp;#128514;&amp;#128514;&amp;#128514;</td>\n",
       "      <td>niggas bitches gotta quit wit da money pics count accurately enough</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24037</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>if a bitch don't like me, somethin' wrong wit da bitch</td>\n",
       "      <td>bitch like somethin wrong wit da bitch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17807</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @TooRacist: A black man asked me \"Hey white boy, do you like niggers?\"\\n\\n\"Well I wouldn't use that word personally\" I said, \"Like is a st&amp;#8230;</td>\n",
       "      <td>black man asked hey white boy like niggers well use word personally said like st</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Funnie watching my nig interview on ESPN</td>\n",
       "      <td>funnie watching nig interview espn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  label  \\\n",
       "1973       3            0                   0        3      2   \n",
       "17144      3            1                   2        0      1   \n",
       "24037      6            0                   6        0      1   \n",
       "17807      3            0                   3        0      1   \n",
       "9348       3            0                   2        1      1   \n",
       "\n",
       "                                                                                                                                                      tweet  \\\n",
       "1973                                          &#9733; BEST ASIAN MASSAGE ON THE park slope --TOP RATED SPA &#9733; 718-622-0221 - 24 http://t.co/Br7el4ZXrw   \n",
       "17144         RT @Runyacheckup: Niggas and bitches gotta quit wit da money pics... If I can count it accurately it ain't enough &#128514;&#128514;&#128514;   \n",
       "24037                                                                                                if a bitch don't like me, somethin' wrong wit da bitch   \n",
       "17807  RT @TooRacist: A black man asked me \"Hey white boy, do you like niggers?\"\\n\\n\"Well I wouldn't use that word personally\" I said, \"Like is a st&#8230;   \n",
       "9348                                                                                                               Funnie watching my nig interview on ESPN   \n",
       "\n",
       "                                                                            clean_tweet  \\\n",
       "1973                                        best asian massage park slope top rated spa   \n",
       "17144               niggas bitches gotta quit wit da money pics count accurately enough   \n",
       "24037                                            bitch like somethin wrong wit da bitch   \n",
       "17807  black man asked hey white boy like niggers well use word personally said like st   \n",
       "9348                                                 funnie watching nig interview espn   \n",
       "\n",
       "       protected_attribute  \n",
       "1973                     1  \n",
       "17144                    1  \n",
       "24037                    0  \n",
       "17807                    1  \n",
       "9348                     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "df['clean_tweet'] = df['tweet'].apply(lambda x: stop(clean_text(x).replace('rt', '')))\n",
    "df = df.rename(columns={'class': 'label'})\n",
    "# df = pd.concat([df, bias_df], axis=1).fillna(0)\n",
    "\n",
    "racial_indicators = racial_indicators + hate_speech_keywords\n",
    "\n",
    "def protected_attributes(text):\n",
    "    tweet_racial_indicators = [indicator for indicator in racial_indicators if indicator in text]\n",
    "\n",
    "    if tweet_racial_indicators:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['protected_attribute'] = df.clean_tweet.apply(lambda tweet: protected_attributes(tweet))\n",
    "\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.872574\n",
       "1    0.127426\n",
       "Name: protected_attribute, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.protected_attribute.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    74.509183\n",
       "0    13.964535\n",
       "2    11.526282\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.protected_attribute==1].label.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    77.85896\n",
       "2    17.56763\n",
       "0     4.57341\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.protected_attribute==0].label.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8915\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.12      0.19       290\n",
      "           1       0.91      0.96      0.94      3832\n",
      "           2       0.85      0.82      0.84       835\n",
      "\n",
      "    accuracy                           0.89      4957\n",
      "   macro avg       0.73      0.64      0.65      4957\n",
      "weighted avg       0.87      0.89      0.88      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ROC-AUC diagnostic\n",
    "\n",
    "def clf(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['clean_tweet'])\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test['clean_tweet'])\n",
    "\n",
    "    # Create the RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the classifier\n",
    "    rf_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf_classifier.predict(X_test_tfidf)\n",
    "    y_pred_proba = rf_classifier.predict_proba(X_test_tfidf)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Print classification report for detailed metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return X_train, X_test, X_train_tfidf, X_test_tfidf, y_train, y_test, rf_classifier, y_pred, y_pred_proba, accuracy\n",
    "\n",
    "X = df[['clean_tweet', 'protected_attribute']]\n",
    "y = df.label\n",
    "\n",
    "X_train, X_test, X_train_tfidf, X_test_tfidf, y_train, y_test, rf_classifier, y_pred, y_pred_proba, accuracy = clf(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8915\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.12      0.19       290\n",
      "           1       0.91      0.96      0.94      3832\n",
      "           2       0.85      0.82      0.84       835\n",
      "\n",
      "    accuracy                           0.89      4957\n",
      "   macro avg       0.73      0.64      0.65      4957\n",
      "weighted avg       0.87      0.89      0.88      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, X_train_tfidf, X_test_tfidf, y_train, y_test, rf_classifier, y_pred, y_pred_proba, accuracy = clf(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8980600595116727\n"
     ]
    }
   ],
   "source": [
    "labels = df.label.sort_values().unique()\n",
    "print(roc_auc_score(y_test, y_pred_proba, multi_class='ovo', labels=labels))\n",
    "\n",
    "overall_auc = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9064\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.20      0.30       198\n",
      "           1       0.92      0.97      0.94      3351\n",
      "           2       0.88      0.81      0.84       776\n",
      "\n",
      "    accuracy                           0.91      4325\n",
      "   macro avg       0.79      0.66      0.69      4325\n",
      "weighted avg       0.90      0.91      0.90      4325\n",
      "\n",
      "0.9211088431536744\n"
     ]
    }
   ],
   "source": [
    "X_privileged = df[df['protected_attribute']==0][['clean_tweet', 'protected_attribute']]\n",
    "y_privileged = df[df['protected_attribute']==0].label\n",
    "\n",
    "X_train, X_test, X_train_tfidf, X_test_tfidf, y_train, y_test, rf_classifier, y_pred, y_pred_proba, accuracy = clf(X_privileged, y_privileged)\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred_proba, multi_class='ovo', labels=labels))\n",
    "\n",
    "privileged_group_AUC = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8481\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.10      0.17        82\n",
      "           1       0.84      0.99      0.91       471\n",
      "           2       0.95      0.78      0.86        79\n",
      "\n",
      "    accuracy                           0.85       632\n",
      "   macro avg       0.84      0.62      0.65       632\n",
      "weighted avg       0.84      0.85      0.81       632\n",
      "\n",
      "0.8910552117494084\n"
     ]
    }
   ],
   "source": [
    "X_unprivileged = df[df['protected_attribute']==1][['clean_tweet', 'protected_attribute']]\n",
    "y_unprivileged = df[df['protected_attribute']==1].label\n",
    "\n",
    "X_train, X_test, X_train_tfidf, X_test_tfidf, y_train, y_test, rf_classifier, y_pred, y_pred_proba, accuracy = clf(X_unprivileged, y_unprivileged)\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred_proba, multi_class='ovo', labels=labels))\n",
    "\n",
    "unprivileged_group_AUC = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(y_true, y_pred, class_labels, multi_class=True, subgroup_mask=None, background_mask=None):\n",
    "    \"\"\"\n",
    "    Calculate AUC for different subsets.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: True labels (binary).\n",
    "    - y_pred: Predicted probabilities or scores.\n",
    "    - subgroup_mask: Mask for the subgroup examples.\n",
    "    - background_mask: Mask for the background examples.\n",
    "\n",
    "    Returns:\n",
    "    - Overall AUC and Bias AUCs (Subgroup AUC, BPSN AUC, BNSP AUC).\n",
    "    \"\"\"\n",
    "\n",
    "    # Overall AUC\n",
    "    if multi_class == False:\n",
    "        overall_auc = roc_auc_score(y_true, y_pred)\n",
    "    else:\n",
    "        overall_auc = roc_auc_score(y_true, y_pred, multi_class='ovo', labels=class_labels)\n",
    "\n",
    "    # Subgroup AUC\n",
    "    if subgroup_mask is not None:\n",
    "        if multi_class == False:\n",
    "            subgroup_auc = roc_auc_score(y_true[subgroup_mask], y_pred[subgroup_mask])\n",
    "        else:\n",
    "            subgroup_auc = roc_auc_score(y_true[subgroup_mask], y_pred[subgroup_mask], multi_class='ovo', labels=class_labels)\n",
    "    else:\n",
    "        subgroup_auc = np.nan\n",
    "\n",
    "    # BPSN AUC\n",
    "    if background_mask is not None and subgroup_mask is not None:\n",
    "        if multi_class == False:\n",
    "            bpsn_mask = np.logical_and(background_mask, ~subgroup_mask)\n",
    "            bpsn_auc = roc_auc_score(y_true[bpsn_mask], y_pred[bpsn_mask])\n",
    "        else:\n",
    "            bpsn_mask = np.logical_and(background_mask, ~subgroup_mask)\n",
    "            bpsn_auc = roc_auc_score(y_true[bpsn_mask], y_pred[bpsn_mask], multi_class='ovo', labels=class_labels)\n",
    "    else:\n",
    "        bpsn_auc = np.nan\n",
    "\n",
    "    # BNSP\n",
    "    if background_mask is not None and subgroup_mask is not None:\n",
    "        if multi_class == False:\n",
    "            bnsp_mask = np.logical_and(~background_mask, subgroup_mask)\n",
    "            bnsp_auc = roc_auc_score(y_true[bnsp_mask], y_pred[bnsp_mask])\n",
    "        else:\n",
    "            bnsp_mask = np.logical_and(~background_mask, subgroup_mask)\n",
    "            bnsp_auc = roc_auc_score(y_true[bnsp_mask], y_pred[bnsp_mask], multi_class='ovo', labels=class_labels)\n",
    "    else:\n",
    "        bnsp_auc = np.nan\n",
    "\n",
    "    return overall_auc, subgroup_auc, bpsn_auc, bnsp_auc\n",
    "\n",
    "# Example usage:\n",
    "# Specify subgroup and background masks\n",
    "subgroup_mask = X_test['protected_attribute'] == 1\n",
    "background_group_mask = X_test['protected_attribute'] == 0\n",
    "\n",
    "# Calculate AUCs\n",
    "overall_auc, subgroup_auc, bpsn_auc, bnsp_auc = calculate_auc(y_test, y_pred_proba, labels, multi_class=True, subgroup_mask=subgroup_mask, background_mask=background_group_mask)\n",
    "\n",
    "# Print the results\n",
    "print(\"Overall AUC:\", overall_auc)\n",
    "print(\"Subgroup AUC:\", subgroup_auc)\n",
    "''' \n",
    "Subgroup AUC: Restrict the test set to examples that mention the identity. \n",
    "A low value means that the model does poorly at distinguishing abusive and non-abusive examples that mention this identity.\n",
    "\n",
    "'''\n",
    "print(\"BPSN AUC:\", bpsn_auc)\n",
    "''' \n",
    "BPSN (Background Positive, Subgroup Negative) AUC: \n",
    "Restrict the test set to non-abusive examples that mention the identity and abusive examples that do not. \n",
    "A low value suggests that the models scores skew higher than they should for examples mentioning this identity.\n",
    "''' \n",
    "print(\"BNSP AUC:\", bnsp_auc)\n",
    "''' \n",
    "BNSP (Background Negative, Subgroup Positive) AUC: \n",
    "Restrict the test set to abusive examples that mention the identity and non-abusive examples that do not. \n",
    "A low value suggests that the models scores skew lower than they should for examples mentioning this identity.\n",
    "''' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC values you provided indicate the performance of your model on different evaluation subsets:\n",
    "\n",
    "1. **Overall AUC (0.8980):**\n",
    "   - This represents the ROC-AUC for the full evaluation set, irrespective of any subgroup or background considerations. A higher overall AUC generally indicates better discrimination between classes.\n",
    "\n",
    "<br>\n",
    "  \n",
    "2. **Subgroup AUC (0.8812):**\n",
    "   - This specifically measures the model's ability to distinguish positive and negative instances within the subgroup. In your case, it might be the AUC for the protected group. A higher value here suggests better performance in discriminating between classes within the subgroup.\n",
    "\n",
    "<br>\n",
    "  \n",
    "3. **BPSN AUC (0.8785):**\n",
    "   - This metric evaluates the model's performance on background samples that are predicted as positive but do not belong to the subgroup. Essentially, it measures how well the model distinguishes between true positives from the subgroup and false positives from the background. A higher BPSN AUC indicates better performance in avoiding false positives on the background group.\n",
    "  \n",
    "<br>\n",
    "\n",
    "4. **BNSP AUC (0.8812):**\n",
    "   - It seems there might be a typo or repetition in the AUC values you provided. Typically, you would have a metric like BNSP (Background Negative, Subgroup Positive) AUC. However, in the values you provided, BNSP AUC has the same value as Subgroup AUC. Please verify whether this is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bias_functions.bias_detection import *\n",
    "\n",
    "# overall_auc, subgroup_auc, bpsn_auc, bnsp_auc, privileged_group_acc, subgroup_accuracy, acc_delta = detect_bias_subgroup(df, 'clean_tweet', 'protected_attribute', multi_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e0139f347092e1ed4847ce2371c15736a968537e473670047b1fb2c4bda7fcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
